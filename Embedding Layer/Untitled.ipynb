{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fc424fd",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "DLL load failed while importing _pywrap_mlir: The specified procedure could not be found.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(tf\u001b[38;5;241m.\u001b[39m__version__)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\__init__.py:37\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_typing\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m module_util \u001b[38;5;28;01mas\u001b[39;00m _module_util\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlazy_loader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LazyLoader \u001b[38;5;28;01mas\u001b[39;00m _LazyLoader\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Make sure code inside the TensorFlow codebase can use tf2.enabled() at import.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\__init__.py:143\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompiler\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mxla\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m xla\n\u001b[0;32m    142\u001b[0m \u001b[38;5;66;03m# MLIR APIs.\u001b[39;00m\n\u001b[1;32m--> 143\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompiler\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmlir\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mlir\n\u001b[0;32m    145\u001b[0m \u001b[38;5;66;03m# Structs (aka extension types)\u001b[39;00m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m extension_type \u001b[38;5;28;01mas\u001b[39;00m _extension_type\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\compiler\\mlir\\mlir.py:17\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# =============================================================================\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"mlir is an experimental library that provides support APIs for MLIR.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pywrap_mlir\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtf_export\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf_export\n\u001b[0;32m     21\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmlir.experimental.convert_graph_def\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert_graph_def\u001b[39m(graph_def,\n\u001b[0;32m     23\u001b[0m                       pass_pipeline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtf-standard-pipeline\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     24\u001b[0m                       show_debug_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_mlir.py:20\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pywrap_tensorflow\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m context\n\u001b[1;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_pywrap_mlir\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mimport_graphdef\u001b[39m(graphdef,\n\u001b[0;32m     24\u001b[0m                     pass_pipeline,\n\u001b[0;32m     25\u001b[0m                     show_debug_info,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     28\u001b[0m                     input_data_shapes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     29\u001b[0m                     output_names\u001b[38;5;241m=\u001b[39m[]):\n\u001b[0;32m     30\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m input_names \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mImportError\u001b[0m: DLL load failed while importing _pywrap_mlir: The specified procedure could not be found."
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4dd507",
   "metadata": {},
   "source": [
    "### Importing one_hot in order to perform one hot encoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c1b6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182deb95",
   "metadata": {},
   "source": [
    "#### Data we are going to use for creating word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83fcb411",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus =[ \"The world is a better place\",\n",
    "      \"Marvel series is my favourite movie\",\n",
    "      \"I like DC movies\",\n",
    "      \"the cat is eating the food\",\n",
    "      \"Tom and Jerry is my favourite movie\",\n",
    "      \"Python is my favourite programming language\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04efa640",
   "metadata": {},
   "source": [
    "### one-hot parameter\n",
    "tf.keras.preprocessing.text.one_hot(\n",
    "    input_text,\n",
    "    \n",
    "    n,\n",
    "    \n",
    "    filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "    \n",
    "    lower=True,\n",
    "    \n",
    "    split=' ',\n",
    "    \n",
    "    analyzer=None\n",
    ")\n",
    "\n",
    "##### Here n is the size of vocabulary we are going to use for our one hot representation \n",
    "So let's define the vocab size:\n",
    "- more vocab size means more feaure captions but high computational cost and adjust it according to the data too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a870c224",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a45f728",
   "metadata": {},
   "source": [
    "### Here we are passing the sentences from our corpus to one hot along with vocab size and in return we are getting the index of the words where are actually exists in the vocab(we define size above)\n",
    "\n",
    "\n",
    "\n",
    "- __look at the below output which tells us that \"the\" appears at 70 index__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f87d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_representation = [one_hot(sentence,vocab_size) for sentence in corpus]\n",
    "one_hot_representation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61c6fea",
   "metadata": {},
   "source": [
    "### Word Embedding Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465cb671",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd4775d",
   "metadata": {},
   "source": [
    "### Padding for NLP\n",
    "All the neural networks require to have inputs that have the same shape and size. However, when we pre-process and use the texts as inputs for our model e.g. LSTM, not all the sentences have the same length. In other words, naturally, some of the sentences are longer or shorter. We need to have the inputs with the same size, this is where the padding is necessary.\n",
    "\n",
    "\n",
    "- __In order to decide the len we are going to use, it's important to explore the data and find the max len of the documents present in the dataset__\n",
    "- __than you can do pre (adding zeros in the start) and post (Adding zeros at the last) padding__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad71d84",
   "metadata": {},
   "source": [
    "**Then we need to do padding, since every sentence in the text has not the same number of words, we can also define maximum number of words for each sentence, if a sentence is longer then we can drop some words. Here we have the lines for padding as illustrated below:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3944beb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_len = 8 ## majorly sentense of size 5 \n",
    "padding_doc = pad_sequences(one_hot_representation, maxlen = sent_len, padding = 'pre')\n",
    "padding_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0892d3cf",
   "metadata": {},
   "source": [
    "## Embedding Layer\n",
    "It performs embedding operations in input layer. It is used to convert positive into dense vectors of fixed size. Its main application is in text analysis. \n",
    "\n",
    "## As we get tense vocab, now we need to convert it into the vectors.\n",
    "\n",
    "### for this, we will be taking each word and than representing it with the number of features (let say we pass the len 8, than each word is represented by 8 different values which are actually the features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d13e257",
   "metadata": {},
   "outputs": [],
   "source": [
    "## feature demensions\n",
    "dim = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d89adc",
   "metadata": {},
   "source": [
    "- __input_dim refers the input dimension.__\n",
    "\n",
    "\n",
    "\n",
    "- __output_dim refers the dimension of the dense embedding.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82010b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(input_dim = vocab_size, output_dim = dim, input_length = sent_len))\n",
    "model.compile(\"adam\", \"mse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d12609",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03f26f5",
   "metadata": {},
   "source": [
    "### feature number is also depends on data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113b8d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "## first vocab_size, second how many features you want for each vector (each index is represented by 10 values)\n",
    "model.add(Embedding(voc_size,10,input_length=sent_length))\n",
    "model.compile('adam','mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be65c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "## one hot doc\n",
    "padding_doc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2f6e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(padding_doc)\n",
    "\n",
    "## Zero is represented by 10 demenions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d7b6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(padding_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d280c63",
   "metadata": {},
   "source": [
    "## Here we Create our own Embedding layer for our Data.\n",
    "## We will be using this to Implement LSTM"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
